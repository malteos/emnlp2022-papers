[{"loc": [0.8197698593139648, -1.438380479812622], "id": 933, "title": "A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction", "authors": "Yuqi Chen, Chen Keming, Xian Sun and Zequn Zhang", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment analysis task that aims to extract triplets of aspect terms, sentiments, and opinion terms from review sentences. Recently, span-level models achieve gratifying results on ASTE task by taking advantage of the predictions of all possible spans.  Since all possible spans significantly increases the number of potential aspect and opinion candidates, it is crucial and challenging to efficiently extract  the triplet elements among them. In this paper, we present a span-level bidirectional network which utilizes all possible spans as input and extracts triplets from spans bidirectionally. Specifically, we devise both the aspect decoder and opinion decoder to decode the span representations and extract triples from aspect-to-opinion and opinion-to-aspect directions. With these two decoders complementing with each other, the whole network can extract triplets from spans more comprehensively. Moreover, considering that mutual exclusion cannot be guaranteed between the spans, we design a similar span separation loss to facilitate the downstream task of distinguishing the correct span by expanding the KL divergence of similar spans during the training process; in the inference process, we adopt an inference strategy to remove conflicting triplets from the results base on their confidence scores. Experimental results show that our framework not only significantly outperforms state-of-the-art methods, but achieves better performance in predicting triplets with multi-token entities and extracting triplets in sentences contain multi-triplets.", "track": "Sentiment Analysis, Stylistic Analysis, and Argument Mining", "label": 35}, {"loc": [-0.011242210865020752, -1.3237720727920532], "id": 323, "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval", "authors": "Houxing Ren, Linjun Shou, Jian Pei, Ning Wu, Ming Gong and Daxin Jiang", "abstract": "Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a large-scale unlabeled corpus. We carefully design a mining method which combines the sparse and dense models to mine the relevance of unlabeled queries and passages. And we introduce a query generator to generate more queries in target languages for unlabeled passages. Through extensive experiments on Mr. TYDI dataset and an industrial dataset from a commercial search engine, we demonstrate that our method performs better than baselines based on various pre-trained multilingual models. Our method even achieves on-par performance with the supervised method on the latter dataset.", "track": "Multilinguality", "label": 28}, {"loc": [0.5190248489379883, -0.5967075824737549], "id": 1031, "title": "Discourse-Aware Soft Prompting for Text Generation", "authors": "Marjan Ghazvininejad, Vladimir Karpukhin, Vera Gor and Asli Celikyilmaz", "abstract": "Current efficient fine-tuning methods\n(e.g., adapters, prefix-tuning, etc.) \nhave optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they don't generalize across all generation tasks. We show that soft-prompt based conditional text generation can be improved with simple and efficient methods that simulate modeling the discourse structure of human written text.\nWe investigate two design choices: \nFirst, we apply hierarchical blocking on the prefix parameters to simulate a higher-level discourse structure of human written text. Second, we apply attention sparsity on the prefix parameters at different layers of the network and learn sparse transformations on the softmax-function. We show that structured design of prefix parameters yields more coherent, faithful and relevant generations than the baseline prefix-tuning on all generation tasks.", "track": "Natural Language Generation", "label": 30}, {"loc": [0.5116285085678101, -0.9790593981742859], "id": 3434, "title": "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations", "authors": "Swarnadeep Saha, Peter Hase, Nazneen Rajani and Mohit Bansal", "abstract": "Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question -- \"Are LLMs and humans equally good at explaining data labels for both easy and hard samples?\" We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.", "track": "Interpretability, Interactivity and Analysis of Models for NLP", "label": 23}, {"loc": [0.2181069403886795, -1.2684440612792969], "id": 2310, "title": "LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models", "authors": "Yeliang Xiu, Zhanhao Xiao and Yongmei Liu", "abstract": "The logical reasoning capabilities of pre-trained language models have recently received much attention. As one of the vital reasoning paradigms, non-monotonic reasoning refers to the fact that conclusions may be invalidated with new information. Existing work has constructed a non-monotonic inference dataset $\\delta$-NLI and explored the performance of language models on it. However, the $\\delta$-NLI dataset is entangled with commonsense reasoning. In this paper, we explore the pure non-monotonic reasoning ability of pre-trained language models. \nWe build a non-monotonic reasoning benchmark, named LogicNMR, with explicit default rules and iterative updates. In the experimental part, the performance of popular language models on LogicNMR is explored from the perspectives of accuracy, generalization, proof-based traceability and robustness. The experimental results show that even though the fine-tuned language models achieve an accuracy of more than 94.4\\% on LogicNMR, they perform unsatisfactorily, with a significant drop, in generalization and proof-based traceability.", "track": "Resources and Evaluation", "label": 33}, {"loc": [1.2265195846557617, -1.0995635986328125], "id": 1609, "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning", "authors": "Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah and Jianfeng Gao", "abstract": "Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning.  We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules -- given the underlying PEFT method of choice -- introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.", "track": "Language Modeling and Analysis of Language Models", "label": 24}, {"loc": [0.5911253690719604, -0.5192480683326721], "id": 3216, "title": "EnDex: Evaluation of Dialogue Engagingness at Scale", "authors": "Guangxuan Xu, Ruibo Liu, Fabrice Harel-Canada, Nischal Reddy Chandra and Nanyun Peng", "abstract": "We propose EnDex, the first human-reaction based model to evaluate dialogue engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED) curated using a novel distant-supervision framework. Engagingness is a key measure that captures high-level quality of AI dialogue systems and closely reflects actual user experience. However, data shortage, plus the abstract and extensive definition of engagingness makes it challenging to develop an automatic metric. Our work departs from mainstream approaches that use synthetic negative examples to train binary classifiers, and instead, proposes a solution using distant-supervision from human-reaction feedback. \nTo support the soundness of our EnDex metric, we offer a theoretical foundation for engagement, an extensive ablation study, and empirical evidence of high correlation on five engagingness related datasets. We will release code, off-the-shelf EnDex model, and a large-scale dataset upon paper publication to facilitate future research.", "track": "Resources and Evaluation", "label": 33}, {"loc": [0.465146541595459, -0.36996251344680786], "id": 774, "title": "Understanding Jargon: Combining Extraction and Generation for Definition Modeling", "authors": "Jie Huang, Hanyin Shao, Kevin Chang, Jinjun Xiong and Wen-mei Hwu", "abstract": "Can machines know what twin prime is? From the composition of this phrase, machines may guess twin prime is a certain kind of prime, but it is still difficult to deduce exactly what twin stands for without additional knowledge. Here, twin prime is a jargon - a specialized term used by experts in a particular field. Explaining jargon is challenging since it usually requires domain knowledge to understand. Recently, there is an increasing interest in extracting and generating definitions of words automatically. However, existing approaches, either extraction or generation, perform poorly on jargon. In this paper, we propose to combine extraction and generation for jargon definition modeling: first extract self- and correlative definitional information of target jargon from the Web and then generate the final definitions by incorporating the extracted definitional information. Our framework is remarkably simple but effective: experiments demonstrate our method can generate high-quality definitions for jargon and outperform state-of-the-art models significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated score from 2.34 to 4.04.", "track": "Semantics: Lexical, Sentence level, Textual Inference and Other areas", "label": 34}, {"loc": [0.8002384901046753, -0.647284984588623], "id": 737, "title": "HydraSum: Disentangling Style Features in Text Summarization with Multi-Decoder Models", "authors": "Tanya Goyal, Nazneen Rajani, Wenhao Liu and Wojciech Kryscinski", "abstract": "Summarization systems make numerous ``decisions'' about summary properties during inference, e.g. degree of copying, specificity and length of outputs, etc. However, these are implicitly encoded within model parameters and specific styles cannot be enforced. To address this, we introduce HydraSum, a new summarization architecture that extends the single decoder framework of current models to a mixture-of-experts version with multiple decoders. We show that HydraSum's multiple decoders automatically learn contrasting summary styles when trained under the standard training objective without any extra supervision. Through experiments on three summarization datasets (CNN, Newsroom and XSum), we show that HydraSum provides a simple mechanism to obtain stylistically-diverse summaries by sampling from either individual decoders or their mixtures, outperforming baseline models. Finally, we demonstrate that a small modification to the gating strategy during training can  enforce an even stricter style partitioning, e.g. high- vs low-abstractiveness or high- vs low-specificity, allowing users to sample from a larger area in the generation space and vary summary styles along multiple dimensions.", "track": "Summarization", "label": 37}, {"loc": [0.22897683084011078, -1.0713651180267334], "id": 1424, "title": "Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation", "authors": "Shoutao Guo, Shaolei Zhang and Yang Feng", "abstract": "Simultaneous machine translation (SiMT) starts its translation before reading the whole source sentence and employs either fixed or adaptive policy to generate the target sentence. Compared to the fixed policy, the adaptive policy achieves better latency-quality tradeoffs by adopting a flexible translation policy. If the policy can evaluate rationality before taking action, the probability of incorrect actions will also decrease. However, previous methods lack evaluation of actions before taking them. In this paper, we propose a method of performing the adaptive policy via integrating post-evaluation into the fixed policy. Specifically, whenever a candidate token is generated, our model will evaluate the rationality of the next action by measuring the change in the source content. Our model will then take different actions based on the evaluation results. Experiments on three translation tasks show that our method can exceed strong baselines under all latency.", "track": "Machine Translation", "label": 27}, {"loc": [-0.052250899374485016, -0.6070519089698792], "id": 518, "title": "Discovering Differences in the Representation of People using Contextualized Semantic Axes", "authors": "Li Lucy, Divya Tadimeti and David Bamman", "abstract": "A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against \"semantic axes\" that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men's communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.", "track": "Computational Social Science and Cultural Analytics", "label": 1}, {"loc": [0.6169381141662598, -1.424986481666565], "id": 4618, "title": "PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings", "authors": "Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng and Daxin Jiang", "abstract": "Learning sentence embeddings in an unsupervised manner is fundamental in natural language processing. Recent common practice is to couple pre-trained language models with unsupervised contrastive learning, whose success relies on augmenting a sentence with a semantically-close positive instance to construct contrastive pairs. Nonetheless, existing approaches usually depend on a mono-augmenting strategy, which causes learning shortcuts towards the augmenting biases and thus corrupts the quality of sentence embeddings. A straightforward solution is resorting to more diverse positives from a multi-augmenting strategy, while an open question remains about how to unsupervisedly learn from the diverse positives but with uneven augmenting qualities in the text field. As one answer, we propose a novel Peer-Contrastive Learning (PCL) with diverse augmentations. PCL constructs diverse contrastive positives and negatives at the group level for unsupervised sentence embeddings. PCL performs peer-positive contrast as well as peer-network cooperation, which offers an inherent anti-bias ability and an effective way to learn from diverse augmentations. Experiments on STS benchmarks verify the effectiveness of PCL against its competitors in unsupervised sentence embeddings.", "track": "Semantics: Lexical, Sentence level, Textual Inference and Other areas", "label": 34}, {"loc": [1.2212141752243042, -0.8343108892440796], "id": 3688, "title": "Sharpness-Aware Minimization with Dynamic Reweighting", "authors": "Wenxuan Zhou, Fangyu Liu, Huan Zhang and Muhao Chen", "abstract": "Deep neural networks are often overparameterized and may not easily achieve model generalization. Adversarial training has shown effectiveness in improving generalization by regularizing the change of loss on top of adversarially chosen perturbations. The recently proposed sharpness-aware minimization (SAM) algorithm conducts adversarial weight perturbation, encouraging the model to converge to a flat minima. SAM finds a common adversarial weight perturbation per-batch. Although per-instance adversarial weight perturbations are stronger adversaries and can potentially lead to better generalization performance, their computational cost is very high and thus it is impossible to use per-instance perturbations efficiently in SAM. In this paper, we tackle this efficiency bottleneck and propose sharpness-aware minimization with dynamic reweighting (delta-SAM). Our theoretical analysis motivates that it is possible to approach the stronger, per-instance adversarial weight perturbations using reweighted per-batch weight perturbations. delta-SAM dynamically reweights perturbation within each batch according to the theoretically principled weighting factors, serving as a good approximation to per-instance perturbation. Experiments on various natural language understanding tasks demonstrate the effectiveness of delta-SAM.", "track": "Machine Learning for NLP", "label": 26}, {"loc": [0.6213061809539795, -0.6900588870048523], "id": 3107, "title": "Improving compositional generalization for multi-step quantitative reasoning in question answering", "authors": "Armineh Nourbakhsh, Cathy Jiao, Sameena Shah and Carolyn Ros\u00e9", "abstract": "Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness of QA models, allowing them to capture arithmetic logic that is expressed verbally. Borrowing from the literature on semantic parsing, we propose a method that encourages the QA models to adjust their attention patterns and capture input/output alignments that are meaningful to the reasoning task. We show how this strategy improves program accuracy and renders the models more robust against overfitting as the number of reasoning steps grows. Our approach is designed as a standalone module which can be prepended to many existing models and trained in an end-to-end fashion without the need for additional supervisory signal. As part of this exercise, we also create a unified dataset building on four previously released numerical QA datasets over tabular data.", "track": "Question Answering", "label": 32}, {"loc": [-0.07309156656265259, -1.3459322452545166], "id": 4451, "title": "A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing", "authors": "Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura and Masaaki Nagata", "abstract": "To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. \nThis paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various transformer-based pre-trained language models.\nThe experimental results obtained from two benchmark datasets demonstrate that the parsing performance strongly relies on the pre-trained language models rather than the parsing strategies.\nIn particular, the bottom-up parser achieves large performance gains compared to the current best parser when employing DeBERTa.\nWe further reveal that language models with a span-masking scheme especially boost the parsing performance through our analysis within intra- and multi-sentential parsing, and nuclearity prediction.", "track": "Discourse and Pragmatics", "label": 3}, {"loc": [1.3344141244888306, -1.0329177379608154], "id": 335, "title": "CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation", "authors": "Jian Yang, Shaohan Huang, Shuming Ma, Yuwei Yin, Li Dong, Dongdong Zhang, hongcheng guo, Zhoujun Li and Furu Wei", "abstract": "Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned cross-lingual representations or machine translation results. However, the performance of cross-lingual NER methods is severely affected by the unsatisfactory quality of translation or label projection. To address these problems, we propose a Cross-lingual Entity Projection framework (CROP) to enable zero-shot cross-lingual NER with the help of a multilingual labeled sequence translation model. Specifically, the target sequence is first translated into the source language and then tagged by a source NER model. We further adopt a labeled sequence translation model to project the tagged sequence back to the target language and label the target raw sentence. Ultimately, the whole pipeline is integrated into an end-to-end model by the way of self-training. Experimental results on two benchmarks demonstrate that our method substantially outperforms the previous strong baseline by a large margin of +3~7 F1 scores and achieves state-of-the-art performance.", "track": "Multilinguality", "label": 28}, {"loc": [0.32818347215652466, -0.091327965259552], "id": 2938, "title": "Towards Summary Candidates Fusion", "authors": "Mathieu Ravaut, Shafiq Joty and Nancy Chen", "abstract": "Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam. Recently, re-ranking methods have been proposed, to learn to select a better summary candidate. However, such methods are limited by the summary quality aspects captured by the first-stage candidates. To bypass this limitation, we propose a new paradigm in second-stage abstractive summarization called SummaFusion that fuses several summary candidates to produce a novel abstractive second-stage summary. Our method works well on several summarization datasets, improving both the ROUGE scores and qualitative properties of fused summaries. It is especially good when the candidates to fuse are worse, such as in the few-shot setup where we set a new state-of-the art. We will make our code and checkpoints available at https://github.com/ntunlp/SummaFusion/.", "track": "Summarization", "label": 37}, {"loc": [0.40497663617134094, -0.8115098476409912], "id": 1626, "title": "T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation", "authors": "Paul-Ambroise Duquenne, Hongyu Gong, Beno\u00eet Sagot and Holger Schwenk", "abstract": "We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.\nDespite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.", "track": "Multilinguality", "label": 28}, {"loc": [1.433632254600525, -1.210536241531372], "id": 1027, "title": "Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation", "authors": "Dongha Lee, Jiaming Shen, Seonghyeon Lee, Susik Yoon, Hwanjo Yu and Jiawei Han", "abstract": "Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global taxonomy structure. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies.", "track": "Information Retrieval and Text Mining", "label": 22}, {"loc": [0.7708336114883423, -0.4267420470714569], "id": 3059, "title": "POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events", "authors": "Sai P. Vallurupalli, Sayontan Ghosh, Katrin Erk, Niranjan Balasubramanian and Francis Ferraro", "abstract": "Knowledge about outcomes is critical for complex event understanding but is hard to acquire.\nWe show that by pre-identifying a participant in a complex event, crowdworkers are able\nto (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement of participants in causing the situation, and (3) ground the\noutcome of the situation in state changes of the participants. By creating a multi-step interface and a careful quality control strategy, we collect a high quality annotated dataset of\n8K short newswire narratives and ROCStories with high inter-annotator agreement (0.74-0.96\nweighted Fleiss Kappa). Our dataset, POQUe (Participant Outcome Questions), enables the\nexploration and development of models that address multiple aspects of semantic understanding. Experimentally, we show that current language models lag behind human performance in subtle ways through our task formulations that target abstract and specific comprehension of a complex event, its outcome, and a participant's influence over the event culmination.", "track": "Ethic Concerns:Resources and Evaluation", "label": 14}]